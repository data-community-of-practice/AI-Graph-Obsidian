Zijian Yang

ORCiD: 0009-0006-8301-7634

------

Unlocking the Role of Large Language Models in Machine Translation Evaluation

**Article link:** https://arxiv.org/abs/2410.03278

ðŸ“ŒThis study dives into the essential components required by large language models (LLMs) for machine translation (MT) evaluation. It explores how reference translations, prompting techniques, and error information impact the quality of MT assessment across various language pairs.

ðŸ”¹Evaluating machine translation has always relied on metrics like BLEU and BERTScore, but recent advancements in LLMs suggest a potential leap in performance. However, the optimal input needed for accurate evaluationâ€”whether source texts, references, or error feedbackâ€”is still under exploration. This paper offers a comprehensive analysis across eight language pairs to address this gap.

ðŸ”¹The research examines different prompting strategies like zero-shot, Chain of Thought (CoT), and few-shot prompting to assess MT quality using LLMs. Interestingly, it finds that larger models are not always superior but benefit more from CoT prompting, a method that helps break down evaluation steps logically.

ðŸ”¹The study reveals that reference translations significantly enhance LLM-based evaluations, particularly in low-resource language pairs. Additionally, while LLMs often outperform traditional models, their inconsistency in providing numerical scores poses challenges for reliable translation assessments.

ðŸ”¹As LLMs continue to evolve, this paper highlights the need for further research into fine-tuning and error explainability to make LLM-based MT evaluations more robust and practical. The findings pave the way for better resource utilization in translation evaluation tasks.

Qian, S., Sindhujan, A., Kabra, M., Kanojia, D., OrÄƒsan, C., Ranasinghe, T., & Blain, F. (2024). What do Large Language Models Need for Machine Translation Evaluation? [ArXiv.org](http://ArXiv.org). https://arxiv.org/abs/2410.03278