AI vs. Ableism: The Gap in Detecting Digital Discrimination

ðŸ“ŒAs online platforms increasingly rely on AI for content moderation, there's a critical need to understand how well these systems identify and interpret ableist content. This study delves into the effectiveness of toxicity classifiers and large language models in recognizing ableist speech, comparing their performance to that of people with disabilities. The research uncovers significant gaps in AI's ability to capture the nuances and emotional impact of ableist language, highlighting the importance of involving disabled voices in the development of content moderation technologies.

Article Link: https://arxiv.org/abs/2410.03448

ðŸ”¹ Online platforms use toxicity classifiers and large language models to moderate harmful content, but their effectiveness in identifying ableist speech is largely unknown. Current AI systems may not capture the nuances of ableism, potentially failing to protect users with disabilities from harmful content while risking over-moderation of disability-related discussions.

ðŸ”¹ This study introduces a novel approach by directly comparing AI systems' performance to that of people with disabilities in identifying ableist content. By analyzing toxicity and ableism ratings, as well as explanations provided by both AI and humans, the research offers unprecedented insights into the gaps between AI and human understanding of ableist speech. This methodology provides a foundation for developing more inclusive and effective content moderation systems.

ðŸ”¹ The study revealed that toxicity classifiers significantly underrated ableist content compared to people with disabilities. While large language models performed better, they still struggled with nuanced forms of ableism. Notably, AI explanations of ableist content lacked the contextual awareness and emotional understanding present in human responses, highlighting a critical gap in AI's ability to interpret the full impact of ableist language.

ðŸ”¹ This research underscores the need to involve people with disabilities in the development and evaluation of AI content moderation systems. Future challenges include creating more representative datasets of ableist speech, developing context-aware AI that can interpret nuanced forms of ableism, and balancing sensitivity to ableist harm with avoiding over-moderation of disability-related content.

Phutane, M., Seelam, A., & Vashistha, A. (2024). How Toxicity Classifiers and Large Language Models Respond to Ableism. arXiv. DOI: 10.48550/arXiv.2410.03448