Zijian Yang

ORCiD: 0009-0006-8301-7634

------

Preventing Dataset Misuse in Fine-Tuning Text-to-Image Models

**Article link:** https://arxiv.org/abs/2409.18897

ðŸ“ŒIn the evolving field of AI, text-to-image models like Stable Diffusion are often fine-tuned using domain-specific datasets to generate images for unique tasks. However, these datasets are at risk of unauthorized usage, leading to potential misuse. This paper introduces a framework to detect and prevent dataset abuse during model fine-tuning for text-to-image synthesis, focusing on preserving the rights of data owners.

ðŸ”¹Generative AI models such as Stable Diffusion, DALLÂ·E 2, and MidJourney have rapidly gained popularity for creating realistic images from text descriptions. While pretrained models excel at generalized tasks, fine-tuning them with specific datasets enables users to create domain-specific outputs. However, dataset owners face the challenge of protecting their data from unauthorized use or sharing by users who may exploit the data for unintended purposes.

ðŸ”¹The paper presents a dataset watermarking framework that allows data owners to detect unauthorized dataset usage. It achieves this by embedding imperceptible watermarks into only 2% of the data, making it highly stealthy and efficient. The framework leverages both token-based and image-based watermarking schemes, enabling the detection of misuse without compromising the quality of the data or the output.

ðŸ”¹The framework demonstrates impressive results, showing that minimal data modification is sufficient to detect misuse with high accuracy. Through various experiments, it proves robust across different datasets and fine-tuning methods, with the ability to trace data leaks and identify unauthorized users. Its effectiveness, harmlessness, and transferability make it a practical solution for large-scale dataset protection.

ðŸ”¹This watermarking framework offers a valuable tool for dataset owners looking to protect their assets while enabling the safe and authorized use of their data in fine-tuning AI models. By ensuring traceability and preventing misuse, it opens new possibilities for secure AI development, though future challenges may include optimizing the system for larger-scale applications and more diverse datasets.

Wang, S., Zhu, Y., Tong, W., & Zhong, S. (2024). Detecting Dataset Abuse in Fine-Tuning Stable Diffusion Models for Text-to-Image Synthesis. [ArXiv.org](http://ArXiv.org). https://arxiv.org/abs/2409.18897