Clouding the Issue: Fooling Satellite Image AI with Natural-Looking Adversarial Attacks

ðŸ“Œ Remote sensing image classification systems, crucial for tasks like urban planning and disaster detection, are vulnerable to adversarial attacks. While traditional methods often produce unnatural perturbations, this paper introduces a novel approach: generating cloud adversarial examples. By simulating realistic cloud patterns using Perlin noise, the authors create deceptive images that fool classifiers while appearing natural to human observers, potentially exposing new vulnerabilities in remote sensing AI systems.

Article Link: https://arxiv.org/abs/2409.14240

ðŸ”¹ Traditional adversarial attacks on remote sensing image classifiers often produce unnatural perturbations, easily detectable by human observers. These methods fail to leverage the unique characteristics of satellite imagery, such as atmospheric effects. This gap leaves unexplored vulnerabilities in remote sensing AI systems, potentially undermining their reliability in critical applications like urban planning and disaster response.

ðŸ”¹ The paper introduces a novel "Perlin Gradient Generator Network" (PGGN) to create realistic cloud patterns as adversarial perturbations. This approach transforms the typically random Perlin noise generation into a controllable, optimizable process. By using a Differential Evolution algorithm, the method achieves a query-based black-box attack, requiring no internal knowledge of the target model. This innovative technique bridges the gap between natural-looking perturbations and effective adversarial attacks in the remote sensing domain.

ðŸ”¹ The proposed method achieved high attack success rates (>90% average) across various neural network architectures on two remote sensing datasets. It demonstrated good query efficiency compared to other black-box methods and showed reasonable transferability across different model architectures. Importantly, the generated adversarial examples appeared more visually plausible than traditional perturbation-based attacks, aligning better with human perception of natural cloud patterns in satellite imagery.

ðŸ”¹ This work opens new avenues for studying the robustness of remote sensing classification systems against natural-looking adversarial attacks. Future challenges include exploring the impact of simulated atmospheric effects on neural network decision boundaries and developing defenses against these more subtle adversarial examples. As AI becomes increasingly crucial in Earth observation applications, understanding and mitigating such vulnerabilities is essential for ensuring reliable and trustworthy systems.

Ma, F., Feng, Y., Zhang, F., & Zhou, Y. (2024). Cloud Adversarial Example Generation for Remote Sensing Image Classification. arXiv. DOI: 10.48550/ARXIV.2409.14240