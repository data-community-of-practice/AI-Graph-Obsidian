Zijian Yang

ORCiD: 0009-0006-8301-7634

------

Challenging AI: Inferring Causality with Vision Large Language Models

ðŸ“ŒThe paper introduces the Multimodal Causal Reasoning (MuCR) benchmark, designed to test the capabilities of Vision Large Language Models (VLLMs) in understanding cause-and-effect relationships using visual cues alone. It explores how these models perform when tasked with complex visual reasoning challenges.

**Article link:** https://arxiv.org/abs/2408.08105

ðŸ”¹The research highlights a growing interest in the causal reasoning capabilities of AI, particularly within Vision Large Language Models. Despite advances in textual causal reasoning, the ability of VLLMs to interpret visual cues and infer causality remains underexplored. The MuCR benchmark addresses this gap by evaluating VLLMs' performance across different levels of visual reasoning.

ðŸ”¹The MuCR benchmark is unique in its approach, using siamese images to embed semantic causality and test the modelsâ€™ ability to connect visual dots. The paper showcases how VLLMs struggle with tasks that require deep visual understanding, especially when relying solely on image-level data to draw conclusions.

ðŸ”¹Key findings reveal that state-of-the-art VLLMs, such as GPT-4V, still fall short in achieving human-level causal reasoning. The models often miss critical visual cues, leading to errors in understanding the cause-and-effect relationships depicted in the images. This underscores the need for more advanced methods in training and evaluating these models.

ðŸ”¹Overall, the study concludes that while VLLMs have made strides, significant challenges remain in enhancing their causal reasoning capabilities. The MuCR benchmark sets the stage for future research aimed at bridging the gap between human and machine understanding of visual causality, with potential applications spanning across AI-driven diagnostics, autonomous systems, and beyond.

Li, Z., Wang, H., Liu, D., Zhang, C., Ma, A., Long, J., & Cai, W. (2024). Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images. [ArXiv.org](http://ArXiv.org). https://arxiv.org/abs/2408.08105