Unraveling LLM Self-Explanations: When AI Attempts to Explain Itself

ðŸ“Œ As large language models (LLMs) grow increasingly common, it is essential to comprehend how they justify their decisions. This paper explores the reliability of LLMs in self-explaining their outputs. By assessing different methods of self-explanation, the study sheds light on the connection between the internal workings of LLMs and their capacity to explain their reasoning, promoting the development of more transparent AI systems.

Article link: https://arxiv.org/abs/2407.14487

ðŸ”¹ Integrating LLMs into critical decision-making processes necessitates the validation of their reasoning abilities. Although LLMs can explain their outputs, the reliability of these self-explanations is questionable. Current evaluation methods focus on human interpretability or correlation with metrics but fail to assess if these explanations accurately reflect the model's internal decision-making.

ðŸ”¹ The paper introduces a thorough method for evaluating how well large language models explain their own decisions, combining two approaches: one that asks the AI to identify important parts of its input, and another that asks it to make small changes that would alter its decision. By comparing these AI-generated explanations to what humans think is important, and to other computer-based methods of understanding AI decisions, the study provides a multi-faceted assessment of explanation reliability. Notably, the research explores the potential of counterfactual explanations as a promising alternative to traditional explainability methods, offering a new path for creating faithful and easily verifiable explanations in LLMs.

ðŸ”¹ The study reveals that LLM self-explanations sometimes correlate with analytic methods, but this correlation is task-dependent and inconsistent. Importantly, self-explanation faithfulness seems to stem from mimicking human explanations, not directly reflecting the model's decision process. The research found no clear advantage of analytic methods over self-explanations in faithfulness.

ðŸ”¹ The research concludes that LLM self-explanation reliability varies significantly with task, model size, and explanation method. It highlights counterfactual explanations as a promising approach for faithful and verifiable LLM explanations. Future work should explore larger models, refine prompting strategies, and bridge the gap between self-explanations and model reasoning. The findings emphasize the need for task-specific evaluation in LLM explainability, challenging one-size-fits-all approaches to ensuring reliable AI explanations across diverse applications.

ðŸ“‘ Randl, K., Pavlopoulos, J., Henriksson, A., & Lindgren, T. (2024). Evaluating the Reliability of Self-Explanations in Large Language Models. arXiv. DOI: 10.48550/arXiv.2407.14487