Unraveling LLM Self-Explanations: When AI Attempts to Explain Itself

ðŸ“Œ As large language models (LLMs) grow increasingly common, it is essential to comprehend how they justify their decisions. This paper explores the reliability of LLMs in self-explaining their outputs. By assessing different methods of self-explanation, the study sheds light on the connection between the internal workings of LLMs and their capacity to explain their reasoning, promoting the development of more transparent AI systems.

Article link: https://arxiv.org/abs/2407.14487

ðŸ”¹ Integrating LLMs into critical decision-making processes necessitates the validation of their reasoning abilities. Although LLMs can explain their outputs, the reliability of these self-explanations is questionable. Current evaluation methods focus on human interpretability or correlation with metrics but fail to assess if these explanations accurately reflect the model's internal decision-making.

ðŸ”¹ The paper introduces a comprehensive evaluation framework for LLM self-explanations, combining both extractive and counterfactual approaches%%simplify the language here so that people without technical knowledge can also understand. So instead of using the terms extractive and counterfactual, try and explain what the approaches do.%%. By comparing these self-explanations to human annotations, gradient-based methods, and attention-based explanations %% explaining gradient based methods and attention based explanations is going to be too lengthy, but see if you can simplify this%%across different tasks and model sizes, the study provides a multi-faceted assessment of explanation reliability. Notably, the research explores the potential of counterfactual explanations as a promising alternative to traditional explainability methods, offering a new path for creating faithful and easily verifiable explanations in LLMs.

ðŸ”¹ The study reveals that LLM self-explanations sometimes correlate with analytic methods, but this correlation is task-dependent and inconsistent. When correlation occurs, it's likely due to both aligning with ground truth rather than each other%%what exactly does this sentence mean?%%. Importantly, self-explanation faithfulness seems to stem from mimicking human explanations, not directly reflecting the model's decision process. The research found no clear advantage of analytic methods over self-explanations in faithfulness.

ðŸ”¹ The research concludes that LLM self-explanation reliability varies significantly with task, model size, and explanation method. It highlights counterfactual explanations as a promising approach for faithful and verifiable LLM explanations. Future work should explore larger models, refine prompting strategies, and bridge the gap between self-explanations and model reasoning. The findings emphasize the need for task-specific evaluation in LLM explainability, challenging one-size-fits-all approaches to ensuring reliable AI explanations across diverse applications.

ðŸ“‘ Randl, K., Pavlopoulos, J., Henriksson, A., & Lindgren, T. (2024). Evaluating the Reliability of Self-Explanations in Large Language Models. arXiv. DOI: 10.48550/arXiv.2407.14487