Large Language Models Know What To Say But Not When To Speak

Author: [Wendi Fan](https://www.linkedin.com/in/wendi-fan-265996310/) (**ORCID: 0000-0003-0284-9166**)

ðŸ“Œ This paper addresses the limitations of current Large Language Models (LLMs) in handling turn-taking during unscripted spoken interactions. It introduces a novel dataset to evaluate LLMs' ability to predict within-turn Transition Relevance Places (TRPs), which are crucial for natural conversation. The study's goal is to improve dialogue systems by making them more capable of responding at appropriate times in conversations, enhancing their timing and interaction flow.

Article link: https://arxiv.org/abs/2410.16044

ðŸ”¹ The key issue discussed is that existing LLMs struggle with identifying opportunities for speaking in unscripted, natural conversations, particularly within turns, where no obvious cues indicate a transition. These limitations affect the smoothness of conversational interactions in dialogue systems.

ðŸ”¹The paper's novelty lies in the creation of a new dataset that captures human-detected within-turn TRPs, offering a unique resource for testing and improving LLMs. This contribution is significant as it provides a foundation for developing more effective spoken dialogue systems by addressing the timing problem in unscripted conversations.

ðŸ”¹The experiments demonstrate that state-of-the-art LLMs, including models like GPT-4 and Mistral, underperform in predicting within-turn TRPs, indicating that current models cannot generalize well to spontaneous spoken interactions.

ðŸ”¹ The authors conclude that while LLMs have made great strides in language tasks, they still require improvements in handling conversational turn-taking. Future challenges include enhancing model performance by incorporating more spoken dialogue data and exploring fine-tuning methods to bridge this gap.

ðŸ“‘ Umair, M., Sarathy, V., De Ruiter, JP. (2024). Large Language Models Know What To Say But Not When To Speak. DOI: 10.48550/arXiv.2410.16044.