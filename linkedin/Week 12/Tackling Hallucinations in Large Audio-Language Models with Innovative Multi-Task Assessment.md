Zijian Yang

ORCiD: 0009-0006-8301-7634

------

Tackling Hallucinations in Large Audio-Language Models with Innovative Multi-Task Assessment

**Article link:** https://arxiv.org/abs/2410.16130

ðŸ“ŒRecent advancements in large audio-language models (LALMs) showcase impressive capabilities in processing audio and text together, but challenges like hallucinationsâ€”incorrect sound detection or misattributionâ€”limit their practical use. Addressing these gaps is crucial for enhancing model reliability, especially in applications like autonomous driving and security systems.

ðŸ”¹The integration of audio perception into large language models opens new possibilities for multimodal understanding. However, existing LALMs often hallucinate sounds that don't exist or fail to recognize temporal sound sequences correctly. This study focuses on identifying these hallucination issues and introduces three tasks to systematically evaluate object existence, temporal order, and sound source attribution in audio inputs.

ðŸ”¹The paper introduces a new framework, Multi-turn and Thoughtful Chain of Hearings (MATCH), designed to improve LALMs' performance in recognizing sound events and their order. By prompting models to generate descriptive audio captions before answering specific questions, MATCH demonstrates significant improvements in model accuracy and consistency, addressing a critical gap in audio processing.

ðŸ”¹Through comprehensive experiments, the study reveals that current models struggle with key tasks like detecting specific sounds and understanding event sequences. The MATCH method improves accuracy across all tasks by 10%-200%, particularly enhancing the detection of object attributes and temporal ordering in audio inputs.

ðŸ”¹The findings suggest that while LALMs have advanced multimodal capabilities, more work is needed to mitigate hallucinations. MATCH offers a promising solution to these challenges, paving the way for more reliable applications in sectors like emergency response, autonomous driving, and beyond.

Kuan, C.-Y., & Lee, H. (2024). Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning. [ArXiv.org](http://ArXiv.org). https://arxiv.org/abs/2410.16130