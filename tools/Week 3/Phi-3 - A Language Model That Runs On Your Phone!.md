
### Introduction

In recent times, the battle for developing the most capable language model has intensified, with many big tech companies getting deeply involved in generative AI. This has led to the creation of massive models, some boasting even trillions of parameters. There is a common misconception that 'bigger is better,' meaning that people often think that the more parameters a model has or the more training data it is fed, the better it will perform. Enter Microsoft with their latest generative AI model, 'Phi-3', that has proved to have better performance on standard open-source benchmarks (that measure the modelâ€™s reasoning ability) like MMLU and MT-bench than popular competitors Mixtral 8x7B and GPT-3.5, which both contain billions more parameters than Phi-3.
### Exploring Phi-3

